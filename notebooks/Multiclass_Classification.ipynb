{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification: 7 Skin Disease Types\n",
    "\n",
    "This notebook performs complete multi-class classification pipeline:\n",
    "1. **Preprocess Data** - Load and prepare images with 7 disease labels\n",
    "2. **Train Model** - Build and train SVM multi-class classifier\n",
    "3. **Evaluate Model** - Assess performance with comprehensive metrics\n",
    "\n",
    "**Classes (7):**\n",
    "- akiec: Actinic keratoses\n",
    "- bcc: Basal cell carcinoma\n",
    "- bkl: Benign keratosis-like lesions\n",
    "- df: Dermatofibroma\n",
    "- mel: Melanoma\n",
    "- nv: Melanocytic nevi\n",
    "- vasc: Vascular lesions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report\n",
    ")\n",
    "from image_loader import (\n",
    "    load_images_from_metadata, verify_image_metadata_match,\n",
    "    get_image_path, load_and_preprocess_image\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_path = '../data/HAM10000_metadata.csv'\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "print(f\"Metadata loaded: {df.shape[0]} samples\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "# Diagnosis mapping\n",
    "dx_dict = {\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'bkl': 'Benign keratosis-like lesions',\n",
    "    'df': 'Dermatofibroma',\n",
    "    'mel': 'Melanoma',\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'vasc': 'Vascular lesions'\n",
    "}\n",
    "\n",
    "print(f\"\\nDiagnosis distribution:\")\n",
    "for dx, count in df['dx'].value_counts().items():\n",
    "    print(f\"  {dx} ({dx_dict[dx]}): {count} ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images (this may take a few minutes)\n",
    "print(\"Loading images... This may take several minutes.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "images, loaded_image_ids = load_images_from_metadata(\n",
    "    df,\n",
    "    base_path='../data',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    normalize=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded images shape: {images.shape}\")\n",
    "print(f\"Value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Memory usage: {images.nbytes / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter metadata and encode labels\n",
    "df_filtered = df[df['image_id'].isin(loaded_image_ids)].reset_index(drop=True)\n",
    "\n",
    "# Encode diagnosis for multi-class classification\n",
    "le_diagnosis = LabelEncoder()\n",
    "y_multiclass = le_diagnosis.fit_transform(df_filtered['dx'])\n",
    "\n",
    "print(f\"Metadata filtered: {len(df_filtered)} samples\")\n",
    "print(f\"\\nNumber of classes: {len(le_diagnosis.classes_)}\")\n",
    "print(f\"Classes: {le_diagnosis.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images,\n",
    "    y_multiclass,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_multiclass\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split:\")\n",
    "print(f\"Training: {X_train.shape}\")\n",
    "print(f\"Testing: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    class_name = le_diagnosis.classes_[label]\n",
    "    print(f\"  {label}: {class_name} - {count} ({count/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples from each class\n",
    "fig, axes = plt.subplots(2, 7, figsize=(20, 6))\n",
    "fig.suptitle('Sample Images from Each Disease Type', fontsize=16)\n",
    "\n",
    "for idx, dx_code in enumerate(le_diagnosis.classes_):\n",
    "    # Find indices for this diagnosis in training set\n",
    "    class_indices = np.where(y_train == idx)[0]\n",
    "    \n",
    "    if len(class_indices) >= 2:\n",
    "        # Show 2 samples\n",
    "        for row in range(2):\n",
    "            sample_idx = class_indices[row]\n",
    "            axes[row, idx].imshow(X_train[sample_idx])\n",
    "            axes[row, idx].set_title(f\"{dx_code}\", fontsize=10)\n",
    "            axes[row, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images for SVM\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Flattened training data: {X_train_flat.shape}\")\n",
    "print(f\"Flattened testing data: {X_test_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVM model for multi-class\n",
    "svm_multiclass = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42,\n",
    "    probability=True,\n",
    "    class_weight='balanced',\n",
    "    decision_function_shape='ovr'  # One-vs-Rest\n",
    ")\n",
    "\n",
    "print(\"SVM Multi-class Model Configuration:\")\n",
    "print(f\"  Kernel: {svm_multiclass.kernel}\")\n",
    "print(f\"  C: {svm_multiclass.C}\")\n",
    "print(f\"  Gamma: {svm_multiclass.gamma}\")\n",
    "print(f\"  Class Weight: {svm_multiclass.class_weight}\")\n",
    "print(f\"  Decision Function: {svm_multiclass.decision_function_shape}\")\n",
    "print(f\"  Number of Classes: {len(le_diagnosis.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import time\n",
    "\n",
    "print(\"Training multi-class SVM model...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "svm_multiclass.fit(X_train_flat, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training complete! Time: {training_time:.2f} seconds\")\n",
    "print(f\"Total support vectors: {sum(svm_multiclass.n_support_)}\")\n",
    "print(f\"Support vectors per class: {dict(zip(range(len(le_diagnosis.classes_)), svm_multiclass.n_support_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = svm_multiclass.predict(X_train_flat)\n",
    "y_test_pred = svm_multiclass.predict(X_test_flat)\n",
    "y_test_proba = svm_multiclass.predict_proba(X_test_flat)\n",
    "\n",
    "print(\"Predictions complete!\")\n",
    "print(f\"\\nSample predictions (first 10):\")\n",
    "for i in range(min(10, len(y_test))):\n",
    "    true_class = le_diagnosis.classes_[y_test[i]]\n",
    "    pred_class = le_diagnosis.classes_[y_test_pred[i]]\n",
    "    match = \"✓\" if y_test[i] == y_test_pred[i] else \"✗\"\n",
    "    print(f\"  {match} True: {true_class:6s} | Predicted: {pred_class:6s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ACCURACY SCORES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "accuracy_diff = train_accuracy - test_accuracy\n",
    "if accuracy_diff > 0.05:\n",
    "    print(f\"\\nWarning: Possible overfitting (diff: {accuracy_diff:.4f})\")\n",
    "else:\n",
    "    print(f\"\\nModel generalizes well (diff: {accuracy_diff:.4f})\")\n",
    "\n",
    "random_accuracy = 1.0 / len(le_diagnosis.classes_)\n",
    "print(f\"\\nRandom guessing: {random_accuracy:.4f} ({random_accuracy*100:.2f}%)\")\n",
    "print(f\"Improvement over random: {(test_accuracy - random_accuracy)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "class_names = le_diagnosis.classes_\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='YlOrRd', cbar=True,\n",
    "            xticklabels=[f\"{cls}\\n({dx_dict[cls][:15]})\" for cls in class_names],\n",
    "            yticklabels=[f\"{cls}\\n({dx_dict[cls][:15]})\" for cls in class_names],\n",
    "            linewidths=0.5, linecolor='gray')\n",
    "plt.title('Multi-class Confusion Matrix (7 Disease Types)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=13)\n",
    "plt.xlabel('Predicted Label', fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/multiclass_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    total_actual = cm_test[i, :].sum()\n",
    "    correct = cm_test[i, i]\n",
    "    class_accuracy = correct / total_actual if total_actual > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{class_name} ({dx_dict[class_name]}):\")\n",
    "    print(f\"  Total samples: {total_actual}\")\n",
    "    print(f\"  Correctly classified: {correct}\")\n",
    "    print(f\"  Class accuracy: {class_accuracy:.4f} ({class_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Most confused with\n",
    "    if total_actual > 0:\n",
    "        misclassified = cm_test[i, :].copy()\n",
    "        misclassified[i] = 0\n",
    "        if misclassified.max() > 0:\n",
    "            most_confused_idx = misclassified.argmax()\n",
    "            most_confused_count = misclassified[most_confused_idx]\n",
    "            print(f\"  Most confused with: {class_names[most_confused_idx]} ({most_confused_count} times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix\n",
    "cm_normalized = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', cbar=True,\n",
    "            xticklabels=[f\"{cls}\\n({dx_dict[cls][:15]})\" for cls in class_names],\n",
    "            yticklabels=[f\"{cls}\\n({dx_dict[cls][:15]})\" for cls in class_names],\n",
    "            linewidths=0.5, linecolor='gray', vmin=0, vmax=1)\n",
    "plt.title('Normalized Confusion Matrix (Percentages)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=13)\n",
    "plt.xlabel('Predicted Label', fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/multiclass_confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Normalized confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    target_names=[f\"{cls} ({dx_dict[cls]})\" for cls in class_names],\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Get report as dictionary\n",
    "report_dict = classification_report(\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    target_names=[f\"{cls}\" for cls in class_names],\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df.to_csv('../results/multiclass_classification_report.csv')\n",
    "print(\"\\nClassification report saved to CSV!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class metrics\n",
    "classes = [f\"{cls}\" for cls in class_names]\n",
    "precision_scores = [report_dict[cls]['precision'] for cls in classes]\n",
    "recall_scores = [report_dict[cls]['recall'] for cls in classes]\n",
    "f1_scores = [report_dict[cls]['f1-score'] for cls in classes]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 14))\n",
    "\n",
    "# Precision\n",
    "axes[0].barh(classes, precision_scores, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Precision', fontsize=12)\n",
    "axes[0].set_title('Precision by Class', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlim([0, 1])\n",
    "for i, v in enumerate(precision_scores):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# Recall\n",
    "axes[1].barh(classes, recall_scores, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_title('Recall by Class', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlim([0, 1])\n",
    "for i, v in enumerate(recall_scores):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# F1-Score\n",
    "axes[2].barh(classes, f1_scores, color='lightgreen', edgecolor='black')\n",
    "axes[2].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[2].set_title('F1-Score by Class', fontsize=13, fontweight='bold')\n",
    "axes[2].set_xlim([0, 1])\n",
    "for i, v in enumerate(f1_scores):\n",
    "    axes[2].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/multiclass_metrics_by_class.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Per-class metrics visualization saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-2 Accuracy\n",
    "top2_correct = 0\n",
    "for i in range(len(y_test)):\n",
    "    top2_pred = y_test_proba[i].argsort()[-2:][::-1]\n",
    "    if y_test[i] in top2_pred:\n",
    "        top2_correct += 1\n",
    "\n",
    "top2_accuracy = top2_correct / len(y_test)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOP-K ACCURACY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Top-1 Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Top-2 Accuracy: {top2_accuracy:.4f} ({top2_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  {test_accuracy*100:.1f}% of predictions are exactly correct\")\n",
    "print(f\"  {top2_accuracy*100:.1f}% have true class in top 2 predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "with open('../models/svm_multiclass_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_multiclass, f)\n",
    "\n",
    "metrics = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'confusion_matrix': cm_test,\n",
    "    'classification_report': report_dict,\n",
    "    'top2_accuracy': top2_accuracy,\n",
    "    'training_time': training_time,\n",
    "    'class_names': class_names.tolist(),\n",
    "    'dx_dict': dx_dict\n",
    "}\n",
    "\n",
    "with open('../models/svm_multiclass_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "\n",
    "# Save label encoder\n",
    "with open('../models/label_encoder_multiclass.pkl', 'wb') as f:\n",
    "    pickle.dump(le_diagnosis, f)\n",
    "\n",
    "print(\"Model, metrics, and label encoder saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"MULTI-CLASS CLASSIFICATION - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel: Support Vector Machine (SVM) - Multi-class\")\n",
    "print(f\"Number of Classes: {len(class_names)}\")\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"Total Support Vectors: {sum(svm_multiclass.n_support_)}\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"  Testing Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  Top-2 Accuracy:    {top2_accuracy:.4f} ({top2_accuracy*100:.2f}%)\")\n",
    "print(f\"  Random Baseline:   {random_accuracy:.4f} ({random_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nBest Performing Classes:\")\n",
    "class_f1_scores = [(cls, report_dict[cls]['f1-score']) for cls in classes]\n",
    "class_f1_scores_sorted = sorted(class_f1_scores, key=lambda x: x[1], reverse=True)\n",
    "for i, (cls, f1) in enumerate(class_f1_scores_sorted[:3], 1):\n",
    "    print(f\"  {i}. {cls} ({dx_dict[cls][:30]}): F1 = {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nWorst Performing Classes:\")\n",
    "for i, (cls, f1) in enumerate(class_f1_scores_sorted[-3:], 1):\n",
    "    print(f\"  {i}. {cls} ({dx_dict[cls][:30]}): F1 = {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nSaved Artifacts:\")\n",
    "print(f\"  - Model: ../models/svm_multiclass_model.pkl\")\n",
    "print(f\"  - Metrics: ../models/svm_multiclass_metrics.pkl\")\n",
    "print(f\"  - Label Encoder: ../models/label_encoder_multiclass.pkl\")\n",
    "print(f\"  - Confusion Matrix: ../results/multiclass_confusion_matrix.png\")\n",
    "print(f\"  - Normalized CM: ../results/multiclass_confusion_matrix_normalized.png\")\n",
    "print(f\"  - Metrics by Class: ../results/multiclass_metrics_by_class.png\")\n",
    "print(f\"  - Report CSV: ../results/multiclass_classification_report.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MULTI-CLASS CLASSIFICATION COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
